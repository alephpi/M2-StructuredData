{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import arff\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# spen\n",
    "from src.multilabel_classification.feature_network import FeatureNetwork\n",
    "from src.multilabel_classification.spen_multilabel import SPENClassification\n",
    "from src.multilabel_classification.utils import (\n",
    "    PATH_MODELS_ML_BIB, PATH_BIBTEX, train_for_num_epochs\n",
    ")\n",
    "from utils_data import get_bibtex, load_data_loader\n",
    "from src.multilabel_classification.feature_network import FILE_FEATURE_NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session on Structured Prediction\n",
    "\n",
    "This practical session is divided in two parts, each one being dedicated to a structured prediction algorithm.\n",
    "\n",
    "- Part 1. Input output kernel regression method (IOKR)\n",
    "- Part 2. Structured prediction energy network (SPEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1] Belanger, David, and Andrew McCallum. \"Structured prediction energy networks.\" International Conference on Machine Learning. PMLR, 2016.\n",
    "\n",
    "[2] Brouard, Céline, Marie Szafranski, and Florence d'Alché-Buc. \"Input output kernel regression: Supervised and semi-supervised structured output prediction with operator-valued kernels.\" Journal of Machine Learning Research 17 (2016).\n",
    "\n",
    "[3] Tsochantaridis, Ioannis, et al. \"Support vector machine learning for interdependent and structured output spaces.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n",
    "\n",
    "[4] Katakis, Ioannis, Grigorios Tsoumakas, and Ioannis Vlahavas. \"Multilabel text classification for automated tag suggestion.\" Proceedings of the ECML/PKDD. Vol. 18. 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured prediction\n",
    "\n",
    "Recall that in structured prediction the goal is to learn a map $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ from an input space $\\mathcal{X}$ to an output space $\\mathcal{Y}$, thanks to a finite set of training points $(x_i, y_i)_{i=1}^n$, that minimizes the following risk\n",
    "\n",
    "$$\\mathbb{E}_{\\rho}[l(f(x), y)]$$\n",
    "\n",
    "for a given loss $l(\\hat y, y)$ defining the discrepancy between a predicted output $\\hat y $ and a true output $y$, and $\\rho$ is an unknown distribution on $\\mathcal{X} \\times \\mathcal{Y}$. In structured prediction we consider **complex output** spaces $\\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibtex dataset\n",
    "\n",
    "You are going to test the two structured prediction methods above on the the bibtex dataset. This is a standard dataset in multi-label classification. Multi-label classification can be seen as a particular instance of structured prediction where $\\mathcal{Y} \\subset \\{0,1\\}^d$.\n",
    "\n",
    "Bibtex is a tag recommendation problem, in which the objective is to propose a relevant set of tags (e.g. url, description, journal volume) to users when they add a new Bibtex entry to the social bookmarking system Bibsonomy (see [4] for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bibtex dataset\n",
    "\n",
    "Y_tr, X_tr, _, _ = get_bibtex(PATH_BIBTEX, use_train=True)\n",
    "Y_te, X_te, _, _ = get_bibtex(PATH_BIBTEX, use_train=False)\n",
    "n_tr = X_tr.shape[0]\n",
    "n_te = X_te.shape[0]\n",
    "input_dim = X_tr.shape[1]\n",
    "label_dim = Y_tr.shape[1]\n",
    "\n",
    "print(f'Train set size = {n_tr}')\n",
    "print(f'Test set size = {n_te}')\n",
    "print(f'Input dim. = {input_dim}')\n",
    "print(f'Output dim. = {label_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition (f1 score).** In this practical session we will evaluate the performances using the f1 score. In multilabel classification there are several ways to define this score. We will use the one corresponding to the sklearn function sklearn.metrics.f1_score(y, y', average=\"samples\").\n",
    "\n",
    "The best f1 scores which have been obtained on the Bibtex dataset are around 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. IOKR for Structured Prediction\n",
    "\n",
    "***\n",
    "\n",
    "In this section, you are going to implement and test an elegant method for structured prediction called **Input Output Kernel Regression (IOKR)**. This method belongs to the family of surrogate methods for structured prediction (cf. lectures).\n",
    "\n",
    "**Kernel methods (quick reminder).** A kernel over a space $\\mathcal{Z}$ is a map $k : \\mathcal{Z} \\times \\mathcal{Z} \\rightarrow \\mathbb{R}$ that can be written:\n",
    "\n",
    "$$k(z, z') = \\langle \\phi(z),\\, \\phi(z')\\rangle_{\\mathcal{G}}$$\n",
    "\n",
    "where $\\mathcal{G}$ is a Hilbert space and $\\phi: \\mathcal{Z} \\rightarrow \\mathcal{G}$ a map. Hence, $k$ define a similarity between any couple of points in $\\mathcal{Z}$. \n",
    "\n",
    "**Two examples of kernel.**\n",
    "\n",
    "The most simple example of kernel is the linear kernel $k(z, z') = \\langle z,\\, z'\\rangle_{\\mathcal{Z}}$. So $\\phi(x) = x$.\n",
    "\n",
    "Another famous kernel is the gaussian kernel, defined, for a given $\\sigma^2 >0$ by: $k(z, z') = \\exp(-\\frac{\\|z-z\\|^2}{2\\sigma^2})$. Here there also exists a $\\phi$ verifying the kernel definition above, but with valued in an infinite dimensional space! This is not an issue as we never need to explicitely compute any $\\phi(x)$ (see below).\n",
    "\n",
    "**Description of the method.** Consider that we are given two kernels, one over the input and another over the output space.\n",
    "\n",
    "$$k_x(x,x') = \\langle \\phi(x),\\, \\phi(x')\\rangle_{\\mathcal{G}}$$\n",
    "\n",
    "$$k_y(y,y') = \\langle \\psi(y),\\, \\psi(y')\\rangle_{\\mathcal{H}}$$ \n",
    "\n",
    "The IOKR method for structured prediction can be described in two steps:\n",
    "\n",
    "- 1/Learning step. Learn the linear map $W : \\mathcal{G} \\times \\mathcal{H}$ thanks to\n",
    "\n",
    "$$\\min_{W} \\sum_{i=1}^n \\|W\\phi(x_i) - \\psi(y_i)\\|^2_{\\mathcal{H}_y} + \\lambda \\|W\\|^2_{F}$$\n",
    "\n",
    "- 2/Testing step. For a given new inputs $x$ predict $\\hat f(x) = \\text{argmin}_{y \\in \\mathcal{Y}}\\; \\langle \\psi(y),\\,\\hat W\\phi(x) \\rangle_{\\mathcal{H}}$\n",
    "\n",
    "Observe that the predictor $\\hat f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is non-linear w.r.t $x$ and also $y$.\n",
    "\n",
    "**Algorithm.** Finally, one can derive a closed-form formula for $\\hat f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ defined above. Here we do not ask you to derive this formula but we give it to you:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f(x) = \\text{argmin}_{y \\in \\mathcal{Y}} \\; \\alpha_x(x)^T M \\alpha_y(y)\n",
    "\\end{equation}\n",
    "\n",
    "with $\\alpha_x(x) = \\left(k_x(x, x_1), \\dots, k_x(x, x_n)\\right) \\in \\mathbb{R}^{n}$, $\\alpha_y(y) = \\left(k_y(y, y_1), \\dots, k_y(y, y_n)\\right) \\in \\mathbb{R}^{n}$, $K_x = \\left(k_x(x_i, x_j)\\right)_{i,j} \\in \\mathbb{R}^{n \\times n}$, and $M = \\left( K_x + \\lambda I_{\\mathbb{R}^n} \\right)^{-1} \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "![](img/IOKR.png)\n",
    "\n",
    "**Remark (link with operator-valued kernel).** The IOKR approach is illustrated on the figure above. Observe that we perform a linear regression from $\\mathcal{G}$ to $\\mathcal{H}$, which can be both infinite dimensional spaces (called reproducing kernel Hilbert space) when $\\phi$ and $\\psi$ are defined with the non linear gaussian kernel. Moreover, notice also, that the approach corresponds to a vector-valued kernel regression from $\\mathcal{X}$ to $\\mathcal{H}$. This situation can be interpreted through the use of an operator-valued kernel (ovk). Here we use the ovk defined by $\\tilde k_x(x,x') =  k_x(x,x') I_{\\mathcal{H}}$, where $I_{\\mathcal{H}}$ denotes the identity operator of $\\mathcal{H}$.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. (decoding) \n",
    "During the decoding step, for a given input $x$ one needs to compute the scores $s(x,y) = \\alpha_x(x)^T M \\alpha_y(y)$ for all $y \\in \\mathcal{Y}$ in order to predict $y$ with the maximum $s(x,y)$. Considering all the $\\{0,1\\}^{159}$ possible 0-1 vectors is too costly. Hence, here we consider only points in the training set Y_tr as possible candidates.\n",
    "\n",
    "Show that, for any test set X_te of size $n_{te}$, you can compute all the $n \\times n_{te}$ scores, by only making matrix multiplications of the three following matrices: $M \\in \\mathbb{R}^{n \\times n}$ (defined above), $K_x = \\left(k_x(x_i, x_j)\\right)_{i,j} \\in \\mathbb{R}^{n \\times n_{te}}$, and $K_y = \\left(k_y(y_i, y_j)\\right)_{i,j} \\in \\mathbb{R}^{n \\times n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. (implementation)\n",
    "Implement the method IOKR using gaussian input and output kernels, with parameters $\\sigma_x^2, \\sigma_y^2 >0$ respectively. Define a python class with a method fit(X_tr, Y_tr, lambda, sigmax, sigmay), and a method predict(X_te). Use the rbf_kernel python function loaded above to compute the gram matrices $K_x, K_y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "class IOKR:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_tr = None\n",
    "        self.Y_tr = None\n",
    "        self.Ky = None\n",
    "        self.sy = None\n",
    "        self.M = None\n",
    "        self.verbose = 0\n",
    "        self.linear = False\n",
    "        \n",
    "    def fit(self, X, Y, L, sx, sy):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_te):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3. (sanity check)\n",
    "Verify that your algorithm is able to learn a predictor that overfits the training set, i.e. such that the f1 score on the train set is close to $1$. For instance, plot the train f1 score w.r.t $\\lambda$ (for $s_x, s_y$ big enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4. (hyperparameters selection)\n",
    "\n",
    "We would like to assess the performance of IOKR on the bibtex dataset.\n",
    "\n",
    "In order to select the hyperparameters $\\lambda, \\sigma_x^2, \\sigma_y^2$, proceed as follows. For a given tuples of parameters, train with $\\frac{4}{5}$ of the train set, and then compute the f1 score (called validation score) on the remaining $\\frac{1}{5}$ of the train set (called validation set). By repeating this over a grid of hyperparameters select the best tuples with the highest validation score.\n",
    "\n",
    "Explain the role of the validation set. Why it is extracted from the train set? Then, compute the test f1 score on the test set, using the best selected parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5. (linear vs non linear)\n",
    "\n",
    "Modify your code such that you can use IOKR with an output linear kernel $k_y(y,y') = \\langle y, y' \\rangle_{\\mathbb{R}^d}$. Then, compare the performance of the linear and the gaussian (output) kernels on the bibtex dataset. For this purpose, you need to select the hyper-parameters ($\\lambda, \\sigma_x^2$ in the case of the linear output kernel, and $\\lambda, \\sigma_x^2, \\sigma_y^2$ for the gaussian kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6. (computational complexity)\n",
    "\n",
    "What is the complexity in time and space of IOKR (for training and decoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Structured prediction energy network (SPEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Prediction Energy Networks (SPENs) are deep energy-based models, constituting a flexible framework for structured prediction, in particular multi-label classification.\n",
    "\n",
    "SPENs tackle multi-label classification, then $\\mathcal{Y} = \\{0,1\\}^L$ with $L \\in \\mathbb{N}^*$. Thus, for a given input $x$, a SPEN performs prediction by solving the following problem::\n",
    "$$\n",
    "    \\underset{y}{\\operatorname{min}} ~ E_x(y) \\quad \\text{subject to} \\quad y \\in \\{0,1\\}^L, \\quad \\text{(1)}\n",
    "$$\n",
    "where enery function $E_x$ is encoded by a deep architecture.\\\\\n",
    "Authors choose to solve an easier problem and optimize over a convex relaxation of the constraint set:\n",
    "$$\n",
    "    \\underset{\\bar{y}}{\\operatorname{min}} ~ E_x(\\bar{y}) \\quad \\text{subject to} \\quad \\bar{y} \\in [0,1]^L. \\quad \\text{(2)}\n",
    "$$\n",
    "Optimization over $[0,1]^L$ to obtain a prediction $\\bar{y}^* \\in [0,1]^L$ can be performed using projected gradient descent or entropic mirror descent. Finally, for an input $x$, prediction $f(x) = (f(x)_1, \\ldots, f(x)_L)^\\top \\in \\{0,1\\}^L$ is obtained by rounding each coordinate $\\bar{y}_i^*$ to $0$ or $1$. Hence, for all $1 \\leq i \\leq L$:\n",
    "$$\n",
    "    f(x)_i = \\left\\{\n",
    "            \\begin{array}{ll}\n",
    "                 0 & \\mbox{if } \\bar{y}_i^* < 0.5\\\\\n",
    "                1 & \\mbox{otherwise.}\n",
    "            \\end{array}\n",
    "            \\right.\n",
    "$$\n",
    "Where $\\bar{y}^* = \\underset{\\bar{y} \\in [0,1]^L}{\\operatorname{argmin}} ~ E_x(\\bar{y})$.\n",
    "\n",
    "A SPEN parameterizes $E_x(\\bar{y})$ as a neural network that takes both $x$ and $\\bar{y}$ as inputs and returns the energy. It consists of two deep architectures as illustrated in the following image:\n",
    "\n",
    "- a feature extraction network;\n",
    "- an energy network.\n",
    "\n",
    "![A SPEN architecture](img/Spen_architecture.png)\n",
    "\n",
    "This practical session is divided in two sections:\n",
    "\n",
    "- A) Learning feature extraction network;\n",
    "- B) Learning energy network.\n",
    "\n",
    "We used an implementation of SPEN in python with PyTorch by Philippe Beardsell and Chih-Chao Hsu (cf. https://github.com/philqc/deep-value-networks-pytorch). Small changes have been made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Feature extraction network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the goal is to learn the feature extractor part of a SPEN architecture encoded by this deep network:\n",
    "$$\n",
    "    F(x) = g(A_2g(A_1x)). \\quad \\text{(3)}\n",
    "$$\n",
    "Hence, by gradient descent and backpropagation we learn the parameters $A_1$ and $A_2$.\n",
    "This training is performed by adding a third layer (which is used only during training and removed later) with a sigmoid activation function and performing a classification task with a Binary Cross Entropy loss.\n",
    "The optimizer used can be rather \"adam\" or \"sgd\" solver.\n",
    "The rest of hyperparameters are indicated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7.  (learning feature extractor network)\n",
    "\n",
    "Use functions train_feature_extraction and test_feature_extraction to train and test feature extraction network and study the influence of optimisation hyperparameters (learning rate, momentum, batch size and number of epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization solver's parameters\n",
    "\n",
    "lr = 1e-5\n",
    "momentum = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor network instantiation\n",
    "\n",
    "f_net = FeatureNetwork(lr=lr, momentum=momentum,\n",
    "                       optimizer=\"adam\", weight_decay=0,\n",
    "                       input_dim=input_dim, label_dim=label_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the feature extractor\n",
    "\n",
    "def train_feature_extraction(X_tr, Y_tr, X_val, Y_val,\n",
    "                             f_net: FeatureNetwork,\n",
    "                             path_save: str, n_epochs: int,\n",
    "                             batch_size: int,\n",
    "                             step_size_scheduler: int,\n",
    "                             norm_inputs=True):\n",
    "    \"\"\"\n",
    "    Trains a feature extractor network\n",
    "    \"\"\"\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    train_loader = load_data_loader(X_tr, Y_tr, use_cuda,\n",
    "                                    batch_size=batch_size,\n",
    "                                    norm_inputs=norm_inputs,\n",
    "                                    shuffle=False)\n",
    "    \n",
    "    valid_loader = load_data_loader(X_val, Y_val, use_cuda,\n",
    "                                    batch_size=batch_size,\n",
    "                                    norm_inputs=norm_inputs,\n",
    "                                    shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        f_net.optimizer, step_size=step_size_scheduler, gamma=0.1\n",
    "    )\n",
    "\n",
    "    loss_train, loss_valid, list_f1_valid = train_for_num_epochs(\n",
    "        f_net,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        os.path.join(path_save, FILE_FEATURE_NETWORK),\n",
    "        n_epochs,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    return loss_train, loss_valid, list_f1_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set in train/validation sets\n",
    "\n",
    "train_valid_ratio=0.8\n",
    "\n",
    "n_train = int(len(X_tr) * train_valid_ratio)\n",
    "indices = list(range(len(X_tr)))\n",
    "\n",
    "x_train, y_train = X_tr[:n_train], Y_tr[:n_train]\n",
    "x_valid, y_valid = X_tr[n_train:], Y_tr[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs and batch size settings\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor training\n",
    "\n",
    "loss_train, loss_valid, list_f1_valid = train_feature_extraction(x_train, y_train,\n",
    "                                                                 x_valid, y_valid,\n",
    "                                                                 f_net,\n",
    "                                                                 PATH_MODELS_ML_BIB,\n",
    "                                                                 n_epochs=n_epochs,\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 step_size_scheduler=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for testing the feature extractor\n",
    "\n",
    "def test_feature_extractor(X_te, Y_te, path_save, f_net):\n",
    "    \"\"\"\n",
    "    Tests a feature extractor network\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    test_loader = load_data_loader(X_te, Y_te, use_cuda)\n",
    "\n",
    "    print('Computing the F1 Score on the test set...')\n",
    "    test_loss, test_mean_f1 = f_net.valid(test_loader)\n",
    "    \n",
    "    return test_loss, test_mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the test f1 score of the feature extractor network\n",
    "\n",
    "test_loss, test_mean_f1 = test_feature_extractor(X_te, Y_te, PATH_MODELS_ML_BIB, f_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Energy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the goal is to learn the energy network part of a SPEN architecture encoded by this deep network:\n",
    "$$\n",
    "    E_x(\\bar{y}) = E(F(x),\\bar{y}) = E_{x}^{\\text {local }}(\\bar{y}) + E_{x}^{\\text {label }}(\\bar{y}) \\quad \\text{(4)}\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "    E_{x}^{\\text {local }}(\\bar{y})=\\sum_{i=1}^{L} \\bar{y}_{i} b_{i}^{\\top} F(x) \\quad \\text{(5)}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    E_{x}^{\\text {label }}(\\bar{y})=c_{2}^{\\top} g\\left(C_{1} \\bar{y}\\right) \\quad \\text{(6)}\n",
    "$$\n",
    "and $F(x)$ is the feature extractor previously trained (with only 2 layers, the third one is now removed as said earlier).\n",
    "Hence, parameers to learn are $b_i$s, $C_1$ and $c_2$.\n",
    "\n",
    "To achieve learning, authors use a structured Hinge loss, similarly to Structured Support Vector Machine (SSVM) (see Lecture 1).\n",
    "\n",
    "Then, they define $\\Delta\\left(y_{p}, y_{g}\\right)$ to be an error function between a prediction $y_{p}$ and the ground truth $y_{g}$ not dependent on the structured output set, such as the Hamming loss. Let $[\\cdot]_{+}=\\max (0, \\cdot) .$ The SSVM minimizes:\n",
    "$$\n",
    "    \\sum_{\\left\\{x_{i}, y_{i}\\right\\}} \\max _{y}\\left[\\Delta\\left(y_{i}, y\\right)-E_{x_{i}}(y)+E_{x_{i}}\\left(y_{i}\\right)\\right]_{+}. \\quad \\text{(7)}\n",
    "$$\n",
    " Learning is performed by minimizing the loss with respect to the parameters of the deep architecture $E_{x}$ using mini-batch stochastic gradient descent. For $\\left\\{x_{i}, y_{i}\\right\\}$, computing the subgradient of (7) with respect to the prediction requires loss-augmented inference:\n",
    "$$\n",
    "     y_{p}=\\underset{y}{\\arg \\min }\\left(-\\Delta\\left(y_{i}, y\\right)+E_{x_{i}}(y)\\right). \\quad \\text{(8)}\n",
    "$$\n",
    "With this, the subgradient of (7) with respect to the model parameters is obtained by back-propagation through $E_{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8.  (role of $E_x^{label}$)\n",
    "\n",
    "What is the role of $E_x^{label}$? Describe breifly its specificity and difference from $E_x^{local}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9.  (learning energy network)\n",
    "\n",
    "Once the feature extraction network is trained, use functions train_energy_network and test_energy_network to train and test energy network and study the influence of :\n",
    "\n",
    "- The size of $c_2$: num_pairwise.\n",
    "- optimisation hyperparameters for network learning (optimisation part associated to learning $b_i$s, $C_1$ and $c_2$): learning rate, momentum, batch size and number of epochs;\n",
    "- optimisation hyperparameters for inference (computation of $\\bar y$): learning rate and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization solver's parameters\n",
    "\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation hyperparameters for loss-augmented inference\n",
    "\n",
    "inf_lr = 0.1\n",
    "momentum_inf = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pairwise x dim label\n",
    "\n",
    "num_pairwise = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy network instantiation\n",
    "\n",
    "feature_extractor_file = \"saved_results/bibtex/feature_network.pth\"\n",
    "\n",
    "spen = SPENClassification(feature_extractor_file, # Loading feature extractor trained earlier\n",
    "                          loss_fn=\"mse\",\n",
    "                          input_dim=input_dim,\n",
    "                          label_dim=label_dim,\n",
    "                          optim=\"adam\",\n",
    "                          learning_rate=learning_rate,\n",
    "                          momentum=momentum,\n",
    "                          non_linearity=nn.Softplus(),\n",
    "                          num_pairwise=num_pairwise\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training SPEN\n",
    "\n",
    "def train_energy_network(X_tr, Y_tr, X_val, Y_val,\n",
    "                         path_save, spen,\n",
    "                         n_epochs=10, batch_size=32,\n",
    "                         norm_inputs=True):\n",
    "    \"\"\"\n",
    "    Trains an energy network\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    train_loader = load_data_loader(X_tr, Y_tr, use_cuda,\n",
    "                                    batch_size=batch_size,\n",
    "                                    norm_inputs=norm_inputs,\n",
    "                                    shuffle=False)\n",
    "    \n",
    "    valid_loader = load_data_loader(X_val, Y_val, use_cuda,\n",
    "                                    batch_size=batch_size,\n",
    "                                    norm_inputs=norm_inputs,\n",
    "                                    shuffle=False)\n",
    "\n",
    "    input_dim = X_tr.shape[1]\n",
    "    label_dim = Y_tr.shape[1]\n",
    "\n",
    "    name = 'spen_bibtex'\n",
    "    path_save_model = os.path.join(path_save, name + '.pth')\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(spen.optimizer, step_size=10, gamma=0.1)\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    loss_train, loss_valid, list_f1_valid = train_for_num_epochs(spen,\n",
    "                                                                 train_loader,\n",
    "                                                                 valid_loader,\n",
    "                                                                 path_save_model,\n",
    "                                                                 n_epochs,\n",
    "                                                                 scheduler)\n",
    "    \n",
    "    return loss_train, loss_valid, list_f1_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs and batch size settings\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPEN training\n",
    "\n",
    "loss_train, loss_valid, list_f1_valid = train_energy_network(x_train, y_train,\n",
    "                                                             x_valid, y_valid,\n",
    "                                                             PATH_MODELS_ML_BIB,\n",
    "                                                             spen, n_epochs=n_epochs,\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             norm_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for testing SPEN\n",
    "\n",
    "def test_energy_network(X_te, Y_te, path_save, spen):\n",
    "    \"\"\"\n",
    "    Tests an energy network\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    test_loader = load_data_loader(X_te, Y_te, use_cuda)\n",
    "\n",
    "    print('Computing the F1 Score on the test set...')\n",
    "    test_loss, test_mean_f1 = spen.valid(test_loader)\n",
    "    \n",
    "    return test_loss, test_mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of SPEN on the test set\n",
    "\n",
    "test_loss, test_mean_f1 = test_energy_network(X_te, Y_te, PATH_MODELS_ML_BIB, spen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10.  ( structured network)\n",
    "\n",
    "Is the structured part of the SPEN's model $E^{label}_x$ useful with the Bibtex dataset? What can you conclude? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
